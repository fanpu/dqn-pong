{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40Yb47zJQglm"
   },
   "source": [
    "DEEP REINFORCEMENT LEARNING EXPLAINED - 15 - 16 - 17\n",
    "# **Deep Q-Network (DQN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q40Fa7qM4_lE"
   },
   "source": [
    "OpenAI Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FA1Y5VCv20XZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\" \n",
    "test_env = gym.make(DEFAULT_ENV_NAME)\n",
    "print(test_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QDaXip14JBv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "print(test_env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uzLQLz04z2i"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzcdmzIL5EMI"
   },
   "source": [
    "\n",
    "Type of hardware accelerator provided by Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjUM99rEKFNt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 20 22:32:05 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 165...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 60%   36C    P8    12W / 100W |   1188MiB /  3908MiB |      1%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       609      G   /usr/lib/Xorg                     262MiB |\r\n",
      "|    0   N/A  N/A      1488      G   /usr/bin/plasmashell               82MiB |\r\n",
      "|    0   N/A  N/A     16853      G   ...AAAAAAAAA= --shared-files      814MiB |\r\n",
      "|    0   N/A  N/A     39852      G   /opt/zoom/zoom                     23MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhmsqgrHikEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pRcuJGVSQi6g"
   },
   "source": [
    "## OpenAI Gym Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPi1lHINMuSu"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
    "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wznv9I1KR_I3"
   },
   "source": [
    "## The DQN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6B8v-Qh5Ykk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn        # Pytorch neural network package\n",
    "import torch.optim as optim  # Pytorch optimization package\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4S1I9xWMkf3"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "colab_type": "code",
    "id": "taYi5LZnIOqz",
    "outputId": "c96d3a1d-ebf6-471a-a93f-c96b479cc9fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_env = make_env(DEFAULT_ENV_NAME)\n",
    "test_net = DQN(test_env.observation_space.shape, test_env.action_space.n).to(device)\n",
    "print(test_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lhv3Yf-aW7UW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPJl73Z1YTa4"
   },
   "source": [
    "Load Tensorboard extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCBQhXLfNeUG"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kb_f_onMXkpb"
   },
   "source": [
    "Import required modules and define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGwHC9dyXoPd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "MEAN_REWARD_BOUND = 19.0           \n",
    "\n",
    "gamma = 0.99                   \n",
    "batch_size = 32                \n",
    "replay_size = 10000            \n",
    "learning_rate = 1e-4           \n",
    "sync_target_frames = 1000      \n",
    "replay_start_size = 10000      \n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay=.999985\n",
    "eps_min=0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FFaMmDKqYmo4"
   },
   "source": [
    "Experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y79CNYsjY4w0"
   },
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQDV04ktY3xs"
   },
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdAKFiMWZw90"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipurwYpa6iKn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training starts at  2021-04-20 22:34:53.354436\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\">>>Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgpmAtchZwM_"
   },
   "source": [
    "Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEoc2PWmM2mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869:  1 games, mean reward -20.000, (epsilon 0.99)\n",
      "Best mean reward updated -20.000\n",
      "1751:  2 games, mean reward -20.500, (epsilon 0.97)\n",
      "2757:  3 games, mean reward -20.333, (epsilon 0.96)\n",
      "3598:  4 games, mean reward -20.250, (epsilon 0.95)\n",
      "4360:  5 games, mean reward -20.400, (epsilon 0.94)\n",
      "5230:  6 games, mean reward -20.333, (epsilon 0.92)\n",
      "6190:  7 games, mean reward -20.286, (epsilon 0.91)\n",
      "6952:  8 games, mean reward -20.375, (epsilon 0.90)\n",
      "7900:  9 games, mean reward -20.333, (epsilon 0.89)\n",
      "8662:  10 games, mean reward -20.400, (epsilon 0.88)\n",
      "9484:  11 games, mean reward -20.455, (epsilon 0.87)\n",
      "10306:  12 games, mean reward -20.500, (epsilon 0.86)\n",
      "11216:  13 games, mean reward -20.538, (epsilon 0.85)\n",
      "12179:  14 games, mean reward -20.571, (epsilon 0.83)\n",
      "13158:  15 games, mean reward -20.533, (epsilon 0.82)\n",
      "14135:  16 games, mean reward -20.438, (epsilon 0.81)\n",
      "15270:  17 games, mean reward -20.353, (epsilon 0.80)\n",
      "16366:  18 games, mean reward -20.333, (epsilon 0.78)\n",
      "17320:  19 games, mean reward -20.368, (epsilon 0.77)\n",
      "18396:  20 games, mean reward -20.350, (epsilon 0.76)\n",
      "19280:  21 games, mean reward -20.381, (epsilon 0.75)\n",
      "20130:  22 games, mean reward -20.409, (epsilon 0.74)\n",
      "20910:  23 games, mean reward -20.435, (epsilon 0.73)\n",
      "22022:  24 games, mean reward -20.458, (epsilon 0.72)\n",
      "22812:  25 games, mean reward -20.480, (epsilon 0.71)\n",
      "23823:  26 games, mean reward -20.462, (epsilon 0.70)\n",
      "24641:  27 games, mean reward -20.481, (epsilon 0.69)\n",
      "25666:  28 games, mean reward -20.429, (epsilon 0.68)\n",
      "26456:  29 games, mean reward -20.448, (epsilon 0.67)\n",
      "27596:  30 games, mean reward -20.400, (epsilon 0.66)\n",
      "28694:  31 games, mean reward -20.387, (epsilon 0.65)\n",
      "30018:  32 games, mean reward -20.312, (epsilon 0.64)\n",
      "31017:  33 games, mean reward -20.333, (epsilon 0.63)\n",
      "31856:  34 games, mean reward -20.353, (epsilon 0.62)\n",
      "32989:  35 games, mean reward -20.314, (epsilon 0.61)\n",
      "34026:  36 games, mean reward -20.333, (epsilon 0.60)\n",
      "35195:  37 games, mean reward -20.351, (epsilon 0.59)\n",
      "36426:  38 games, mean reward -20.342, (epsilon 0.58)\n",
      "37518:  39 games, mean reward -20.308, (epsilon 0.57)\n",
      "38669:  40 games, mean reward -20.250, (epsilon 0.56)\n",
      "40013:  41 games, mean reward -20.220, (epsilon 0.55)\n",
      "41238:  42 games, mean reward -20.167, (epsilon 0.54)\n",
      "42717:  43 games, mean reward -20.070, (epsilon 0.53)\n",
      "43932:  44 games, mean reward -20.045, (epsilon 0.52)\n",
      "45463:  45 games, mean reward -19.978, (epsilon 0.51)\n",
      "Best mean reward updated -19.978\n",
      "46723:  46 games, mean reward -19.978, (epsilon 0.50)\n",
      "47945:  47 games, mean reward -19.957, (epsilon 0.49)\n",
      "Best mean reward updated -19.957\n",
      "49449:  48 games, mean reward -19.938, (epsilon 0.48)\n",
      "Best mean reward updated -19.938\n",
      "50671:  49 games, mean reward -19.939, (epsilon 0.47)\n",
      "51939:  50 games, mean reward -19.920, (epsilon 0.46)\n",
      "Best mean reward updated -19.920\n",
      "53409:  51 games, mean reward -19.863, (epsilon 0.45)\n",
      "Best mean reward updated -19.863\n",
      "54706:  52 games, mean reward -19.827, (epsilon 0.44)\n",
      "Best mean reward updated -19.827\n",
      "56675:  53 games, mean reward -19.717, (epsilon 0.43)\n",
      "Best mean reward updated -19.717\n",
      "58231:  54 games, mean reward -19.667, (epsilon 0.42)\n",
      "Best mean reward updated -19.667\n",
      "59529:  55 games, mean reward -19.673, (epsilon 0.41)\n",
      "60979:  56 games, mean reward -19.607, (epsilon 0.40)\n",
      "Best mean reward updated -19.607\n",
      "62305:  57 games, mean reward -19.579, (epsilon 0.39)\n",
      "Best mean reward updated -19.579\n",
      "63586:  58 games, mean reward -19.569, (epsilon 0.39)\n",
      "Best mean reward updated -19.569\n",
      "65096:  59 games, mean reward -19.525, (epsilon 0.38)\n",
      "Best mean reward updated -19.525\n",
      "66909:  60 games, mean reward -19.500, (epsilon 0.37)\n",
      "Best mean reward updated -19.500\n",
      "68662:  61 games, mean reward -19.459, (epsilon 0.36)\n",
      "Best mean reward updated -19.459\n",
      "70312:  62 games, mean reward -19.387, (epsilon 0.35)\n",
      "Best mean reward updated -19.387\n",
      "71513:  63 games, mean reward -19.413, (epsilon 0.34)\n",
      "72757:  64 games, mean reward -19.422, (epsilon 0.34)\n",
      "74251:  65 games, mean reward -19.446, (epsilon 0.33)\n",
      "76259:  66 games, mean reward -19.424, (epsilon 0.32)\n",
      "77748:  67 games, mean reward -19.373, (epsilon 0.31)\n",
      "Best mean reward updated -19.373\n",
      "79128:  68 games, mean reward -19.353, (epsilon 0.31)\n",
      "Best mean reward updated -19.353\n",
      "80823:  69 games, mean reward -19.319, (epsilon 0.30)\n",
      "Best mean reward updated -19.319\n",
      "82743:  70 games, mean reward -19.243, (epsilon 0.29)\n",
      "Best mean reward updated -19.243\n",
      "84387:  71 games, mean reward -19.211, (epsilon 0.28)\n",
      "Best mean reward updated -19.211\n",
      "86253:  72 games, mean reward -19.139, (epsilon 0.27)\n",
      "Best mean reward updated -19.139\n",
      "87744:  73 games, mean reward -19.137, (epsilon 0.27)\n",
      "Best mean reward updated -19.137\n",
      "89367:  74 games, mean reward -19.122, (epsilon 0.26)\n",
      "Best mean reward updated -19.122\n",
      "91349:  75 games, mean reward -19.080, (epsilon 0.25)\n",
      "Best mean reward updated -19.080\n",
      "92873:  76 games, mean reward -19.079, (epsilon 0.25)\n",
      "Best mean reward updated -19.079\n",
      "94498:  77 games, mean reward -19.078, (epsilon 0.24)\n",
      "Best mean reward updated -19.078\n",
      "96779:  78 games, mean reward -18.974, (epsilon 0.23)\n",
      "Best mean reward updated -18.974\n",
      "98538:  79 games, mean reward -18.937, (epsilon 0.23)\n",
      "Best mean reward updated -18.937\n",
      "100465:  80 games, mean reward -18.887, (epsilon 0.22)\n",
      "Best mean reward updated -18.887\n",
      "102551:  81 games, mean reward -18.815, (epsilon 0.21)\n",
      "Best mean reward updated -18.815\n",
      "104447:  82 games, mean reward -18.793, (epsilon 0.21)\n",
      "Best mean reward updated -18.793\n",
      "106968:  83 games, mean reward -18.747, (epsilon 0.20)\n",
      "Best mean reward updated -18.747\n",
      "108704:  84 games, mean reward -18.750, (epsilon 0.20)\n",
      "110745:  85 games, mean reward -18.706, (epsilon 0.19)\n",
      "Best mean reward updated -18.706\n",
      "112791:  86 games, mean reward -18.686, (epsilon 0.18)\n",
      "Best mean reward updated -18.686\n",
      "114832:  87 games, mean reward -18.632, (epsilon 0.18)\n",
      "Best mean reward updated -18.632\n",
      "116713:  88 games, mean reward -18.625, (epsilon 0.17)\n",
      "Best mean reward updated -18.625\n",
      "118744:  89 games, mean reward -18.573, (epsilon 0.17)\n",
      "Best mean reward updated -18.573\n",
      "120594:  90 games, mean reward -18.567, (epsilon 0.16)\n",
      "Best mean reward updated -18.567\n",
      "122405:  91 games, mean reward -18.538, (epsilon 0.16)\n",
      "Best mean reward updated -18.538\n",
      "125275:  92 games, mean reward -18.511, (epsilon 0.15)\n",
      "Best mean reward updated -18.511\n",
      "127739:  93 games, mean reward -18.484, (epsilon 0.15)\n",
      "Best mean reward updated -18.484\n",
      "129840:  94 games, mean reward -18.489, (epsilon 0.14)\n",
      "132119:  95 games, mean reward -18.463, (epsilon 0.14)\n",
      "Best mean reward updated -18.463\n",
      "134494:  96 games, mean reward -18.438, (epsilon 0.13)\n",
      "Best mean reward updated -18.438\n",
      "137006:  97 games, mean reward -18.392, (epsilon 0.13)\n",
      "Best mean reward updated -18.392\n",
      "138828:  98 games, mean reward -18.398, (epsilon 0.12)\n",
      "141954:  99 games, mean reward -18.313, (epsilon 0.12)\n",
      "Best mean reward updated -18.313\n",
      "143996:  100 games, mean reward -18.280, (epsilon 0.12)\n",
      "Best mean reward updated -18.280\n",
      "146821:  101 games, mean reward -18.190, (epsilon 0.11)\n",
      "Best mean reward updated -18.190\n",
      "149326:  102 games, mean reward -18.090, (epsilon 0.11)\n",
      "Best mean reward updated -18.090\n",
      "151982:  103 games, mean reward -18.000, (epsilon 0.10)\n",
      "Best mean reward updated -18.000\n",
      "154388:  104 games, mean reward -17.920, (epsilon 0.10)\n",
      "Best mean reward updated -17.920\n",
      "157336:  105 games, mean reward -17.840, (epsilon 0.09)\n",
      "Best mean reward updated -17.840\n",
      "160030:  106 games, mean reward -17.760, (epsilon 0.09)\n",
      "Best mean reward updated -17.760\n",
      "162872:  107 games, mean reward -17.650, (epsilon 0.09)\n",
      "Best mean reward updated -17.650\n",
      "165746:  108 games, mean reward -17.510, (epsilon 0.08)\n",
      "Best mean reward updated -17.510\n",
      "168825:  109 games, mean reward -17.400, (epsilon 0.08)\n",
      "Best mean reward updated -17.400\n",
      "171964:  110 games, mean reward -17.230, (epsilon 0.08)\n",
      "Best mean reward updated -17.230\n",
      "174224:  111 games, mean reward -17.150, (epsilon 0.07)\n",
      "Best mean reward updated -17.150\n",
      "177389:  112 games, mean reward -17.010, (epsilon 0.07)\n",
      "Best mean reward updated -17.010\n",
      "180256:  113 games, mean reward -16.720, (epsilon 0.07)\n",
      "Best mean reward updated -16.720\n",
      "183092:  114 games, mean reward -16.430, (epsilon 0.06)\n",
      "Best mean reward updated -16.430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186428:  115 games, mean reward -16.190, (epsilon 0.06)\n",
      "Best mean reward updated -16.190\n",
      "189336:  116 games, mean reward -15.910, (epsilon 0.06)\n",
      "Best mean reward updated -15.910\n",
      "191818:  117 games, mean reward -15.620, (epsilon 0.06)\n",
      "Best mean reward updated -15.620\n",
      "193976:  118 games, mean reward -15.270, (epsilon 0.05)\n",
      "Best mean reward updated -15.270\n",
      "196770:  119 games, mean reward -14.970, (epsilon 0.05)\n",
      "Best mean reward updated -14.970\n",
      "199803:  120 games, mean reward -14.710, (epsilon 0.05)\n",
      "Best mean reward updated -14.710\n",
      "202334:  121 games, mean reward -14.420, (epsilon 0.05)\n",
      "Best mean reward updated -14.420\n",
      "204472:  122 games, mean reward -14.050, (epsilon 0.05)\n",
      "Best mean reward updated -14.050\n",
      "207042:  123 games, mean reward -13.720, (epsilon 0.04)\n",
      "Best mean reward updated -13.720\n",
      "210108:  124 games, mean reward -13.480, (epsilon 0.04)\n",
      "Best mean reward updated -13.480\n",
      "212050:  125 games, mean reward -13.100, (epsilon 0.04)\n",
      "Best mean reward updated -13.100\n",
      "214573:  126 games, mean reward -12.800, (epsilon 0.04)\n",
      "Best mean reward updated -12.800\n",
      "218082:  127 games, mean reward -12.600, (epsilon 0.04)\n",
      "Best mean reward updated -12.600\n",
      "220319:  128 games, mean reward -12.270, (epsilon 0.04)\n",
      "Best mean reward updated -12.270\n",
      "223122:  129 games, mean reward -11.960, (epsilon 0.04)\n",
      "Best mean reward updated -11.960\n",
      "225431:  130 games, mean reward -11.660, (epsilon 0.03)\n",
      "Best mean reward updated -11.660\n",
      "228686:  131 games, mean reward -11.350, (epsilon 0.03)\n",
      "Best mean reward updated -11.350\n",
      "231173:  132 games, mean reward -11.050, (epsilon 0.03)\n",
      "Best mean reward updated -11.050\n",
      "233026:  133 games, mean reward -10.660, (epsilon 0.03)\n",
      "Best mean reward updated -10.660\n",
      "235476:  134 games, mean reward -10.320, (epsilon 0.03)\n",
      "Best mean reward updated -10.320\n",
      "237754:  135 games, mean reward -9.980, (epsilon 0.03)\n",
      "Best mean reward updated -9.980\n",
      "239856:  136 games, mean reward -9.610, (epsilon 0.03)\n",
      "Best mean reward updated -9.610\n",
      "242042:  137 games, mean reward -9.280, (epsilon 0.03)\n",
      "Best mean reward updated -9.280\n",
      "244296:  138 games, mean reward -8.960, (epsilon 0.03)\n",
      "Best mean reward updated -8.960\n",
      "246293:  139 games, mean reward -8.620, (epsilon 0.02)\n",
      "Best mean reward updated -8.620\n",
      "248174:  140 games, mean reward -8.270, (epsilon 0.02)\n",
      "Best mean reward updated -8.270\n",
      "250018:  141 games, mean reward -7.890, (epsilon 0.02)\n",
      "Best mean reward updated -7.890\n",
      "252223:  142 games, mean reward -7.580, (epsilon 0.02)\n",
      "Best mean reward updated -7.580\n",
      "254729:  143 games, mean reward -7.290, (epsilon 0.02)\n",
      "Best mean reward updated -7.290\n",
      "256874:  144 games, mean reward -6.930, (epsilon 0.02)\n",
      "Best mean reward updated -6.930\n",
      "258739:  145 games, mean reward -6.570, (epsilon 0.02)\n",
      "Best mean reward updated -6.570\n",
      "260578:  146 games, mean reward -6.170, (epsilon 0.02)\n",
      "Best mean reward updated -6.170\n",
      "263052:  147 games, mean reward -5.860, (epsilon 0.02)\n",
      "Best mean reward updated -5.860\n",
      "265137:  148 games, mean reward -5.520, (epsilon 0.02)\n",
      "Best mean reward updated -5.520\n",
      "267728:  149 games, mean reward -5.180, (epsilon 0.02)\n",
      "Best mean reward updated -5.180\n",
      "269851:  150 games, mean reward -4.820, (epsilon 0.02)\n",
      "Best mean reward updated -4.820\n",
      "272127:  151 games, mean reward -4.490, (epsilon 0.02)\n",
      "Best mean reward updated -4.490\n",
      "274303:  152 games, mean reward -4.220, (epsilon 0.02)\n",
      "Best mean reward updated -4.220\n",
      "276956:  153 games, mean reward -3.970, (epsilon 0.02)\n",
      "Best mean reward updated -3.970\n",
      "278976:  154 games, mean reward -3.620, (epsilon 0.02)\n",
      "Best mean reward updated -3.620\n",
      "280978:  155 games, mean reward -3.240, (epsilon 0.02)\n",
      "Best mean reward updated -3.240\n",
      "283281:  156 games, mean reward -2.930, (epsilon 0.02)\n",
      "Best mean reward updated -2.930\n",
      "285117:  157 games, mean reward -2.560, (epsilon 0.02)\n",
      "Best mean reward updated -2.560\n",
      "286817:  158 games, mean reward -2.160, (epsilon 0.02)\n",
      "Best mean reward updated -2.160\n",
      "289171:  159 games, mean reward -1.850, (epsilon 0.02)\n",
      "Best mean reward updated -1.850\n",
      "291048:  160 games, mean reward -1.490, (epsilon 0.02)\n",
      "Best mean reward updated -1.490\n",
      "292741:  161 games, mean reward -1.110, (epsilon 0.02)\n",
      "Best mean reward updated -1.110\n",
      "294639:  162 games, mean reward -0.770, (epsilon 0.02)\n",
      "Best mean reward updated -0.770\n",
      "296956:  163 games, mean reward -0.420, (epsilon 0.02)\n",
      "Best mean reward updated -0.420\n",
      "298734:  164 games, mean reward -0.030, (epsilon 0.02)\n",
      "Best mean reward updated -0.030\n",
      "300638:  165 games, mean reward 0.350, (epsilon 0.02)\n",
      "Best mean reward updated 0.350\n",
      "302418:  166 games, mean reward 0.730, (epsilon 0.02)\n",
      "Best mean reward updated 0.730\n",
      "304238:  167 games, mean reward 1.080, (epsilon 0.02)\n",
      "Best mean reward updated 1.080\n",
      "306074:  168 games, mean reward 1.430, (epsilon 0.02)\n",
      "Best mean reward updated 1.430\n",
      "307993:  169 games, mean reward 1.770, (epsilon 0.02)\n",
      "Best mean reward updated 1.770\n",
      "309802:  170 games, mean reward 2.100, (epsilon 0.02)\n",
      "Best mean reward updated 2.100\n",
      "311546:  171 games, mean reward 2.460, (epsilon 0.02)\n",
      "Best mean reward updated 2.460\n",
      "313420:  172 games, mean reward 2.780, (epsilon 0.02)\n",
      "Best mean reward updated 2.780\n",
      "315150:  173 games, mean reward 3.170, (epsilon 0.02)\n",
      "Best mean reward updated 3.170\n",
      "317163:  174 games, mean reward 3.500, (epsilon 0.02)\n",
      "Best mean reward updated 3.500\n",
      "319196:  175 games, mean reward 3.820, (epsilon 0.02)\n",
      "Best mean reward updated 3.820\n",
      "321684:  176 games, mean reward 4.120, (epsilon 0.02)\n",
      "Best mean reward updated 4.120\n",
      "324325:  177 games, mean reward 4.370, (epsilon 0.02)\n",
      "Best mean reward updated 4.370\n",
      "326441:  178 games, mean reward 4.630, (epsilon 0.02)\n",
      "Best mean reward updated 4.630\n",
      "328844:  179 games, mean reward 4.910, (epsilon 0.02)\n",
      "Best mean reward updated 4.910\n",
      "330500:  180 games, mean reward 5.270, (epsilon 0.02)\n",
      "Best mean reward updated 5.270\n",
      "332495:  181 games, mean reward 5.570, (epsilon 0.02)\n",
      "Best mean reward updated 5.570\n",
      "334165:  182 games, mean reward 5.940, (epsilon 0.02)\n",
      "Best mean reward updated 5.940\n",
      "335902:  183 games, mean reward 6.290, (epsilon 0.02)\n",
      "Best mean reward updated 6.290\n",
      "338324:  184 games, mean reward 6.610, (epsilon 0.02)\n",
      "Best mean reward updated 6.610\n",
      "340176:  185 games, mean reward 6.940, (epsilon 0.02)\n",
      "Best mean reward updated 6.940\n",
      "341904:  186 games, mean reward 7.300, (epsilon 0.02)\n",
      "Best mean reward updated 7.300\n",
      "344062:  187 games, mean reward 7.600, (epsilon 0.02)\n",
      "Best mean reward updated 7.600\n",
      "345951:  188 games, mean reward 7.970, (epsilon 0.02)\n",
      "Best mean reward updated 7.970\n",
      "348355:  189 games, mean reward 8.220, (epsilon 0.02)\n",
      "Best mean reward updated 8.220\n",
      "350136:  190 games, mean reward 8.600, (epsilon 0.02)\n",
      "Best mean reward updated 8.600\n",
      "351851:  191 games, mean reward 8.960, (epsilon 0.02)\n",
      "Best mean reward updated 8.960\n",
      "353662:  192 games, mean reward 9.320, (epsilon 0.02)\n",
      "Best mean reward updated 9.320\n",
      "355464:  193 games, mean reward 9.670, (epsilon 0.02)\n",
      "Best mean reward updated 9.670\n",
      "357191:  194 games, mean reward 10.060, (epsilon 0.02)\n",
      "Best mean reward updated 10.060\n",
      "359129:  195 games, mean reward 10.400, (epsilon 0.02)\n",
      "Best mean reward updated 10.400\n",
      "361384:  196 games, mean reward 10.700, (epsilon 0.02)\n",
      "Best mean reward updated 10.700\n",
      "363229:  197 games, mean reward 11.030, (epsilon 0.02)\n",
      "Best mean reward updated 11.030\n",
      "365058:  198 games, mean reward 11.410, (epsilon 0.02)\n",
      "Best mean reward updated 11.410\n",
      "367070:  199 games, mean reward 11.690, (epsilon 0.02)\n",
      "Best mean reward updated 11.690\n",
      "368767:  200 games, mean reward 12.050, (epsilon 0.02)\n",
      "Best mean reward updated 12.050\n",
      "370988:  201 games, mean reward 12.310, (epsilon 0.02)\n",
      "Best mean reward updated 12.310\n",
      "372997:  202 games, mean reward 12.590, (epsilon 0.02)\n",
      "Best mean reward updated 12.590\n",
      "374890:  203 games, mean reward 12.890, (epsilon 0.02)\n",
      "Best mean reward updated 12.890\n",
      "376717:  204 games, mean reward 13.190, (epsilon 0.02)\n",
      "Best mean reward updated 13.190\n",
      "378743:  205 games, mean reward 13.500, (epsilon 0.02)\n",
      "Best mean reward updated 13.500\n",
      "380977:  206 games, mean reward 13.760, (epsilon 0.02)\n",
      "Best mean reward updated 13.760\n",
      "382785:  207 games, mean reward 14.050, (epsilon 0.02)\n",
      "Best mean reward updated 14.050\n",
      "384542:  208 games, mean reward 14.320, (epsilon 0.02)\n",
      "Best mean reward updated 14.320\n",
      "386427:  209 games, mean reward 14.600, (epsilon 0.02)\n",
      "Best mean reward updated 14.600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388178:  210 games, mean reward 14.840, (epsilon 0.02)\n",
      "Best mean reward updated 14.840\n",
      "390077:  211 games, mean reward 15.140, (epsilon 0.02)\n",
      "Best mean reward updated 15.140\n",
      "391990:  212 games, mean reward 15.400, (epsilon 0.02)\n",
      "Best mean reward updated 15.400\n",
      "393750:  213 games, mean reward 15.520, (epsilon 0.02)\n",
      "Best mean reward updated 15.520\n",
      "395745:  214 games, mean reward 15.600, (epsilon 0.02)\n",
      "Best mean reward updated 15.600\n",
      "397647:  215 games, mean reward 15.740, (epsilon 0.02)\n",
      "Best mean reward updated 15.740\n",
      "399287:  216 games, mean reward 15.860, (epsilon 0.02)\n",
      "Best mean reward updated 15.860\n",
      "400995:  217 games, mean reward 15.950, (epsilon 0.02)\n",
      "Best mean reward updated 15.950\n",
      "403178:  218 games, mean reward 15.940, (epsilon 0.02)\n",
      "405120:  219 games, mean reward 16.020, (epsilon 0.02)\n",
      "Best mean reward updated 16.020\n",
      "407541:  220 games, mean reward 16.080, (epsilon 0.02)\n",
      "Best mean reward updated 16.080\n",
      "409471:  221 games, mean reward 16.170, (epsilon 0.02)\n",
      "Best mean reward updated 16.170\n",
      "411450:  222 games, mean reward 16.180, (epsilon 0.02)\n",
      "Best mean reward updated 16.180\n",
      "413532:  223 games, mean reward 16.200, (epsilon 0.02)\n",
      "Best mean reward updated 16.200\n",
      "415506:  224 games, mean reward 16.340, (epsilon 0.02)\n",
      "Best mean reward updated 16.340\n",
      "417140:  225 games, mean reward 16.380, (epsilon 0.02)\n",
      "Best mean reward updated 16.380\n",
      "419069:  226 games, mean reward 16.450, (epsilon 0.02)\n",
      "Best mean reward updated 16.450\n",
      "420858:  227 games, mean reward 16.650, (epsilon 0.02)\n",
      "Best mean reward updated 16.650\n",
      "422735:  228 games, mean reward 16.690, (epsilon 0.02)\n",
      "Best mean reward updated 16.690\n",
      "424405:  229 games, mean reward 16.790, (epsilon 0.02)\n",
      "Best mean reward updated 16.790\n",
      "426382:  230 games, mean reward 16.860, (epsilon 0.02)\n",
      "Best mean reward updated 16.860\n",
      "428050:  231 games, mean reward 16.950, (epsilon 0.02)\n",
      "Best mean reward updated 16.950\n",
      "429891:  232 games, mean reward 17.020, (epsilon 0.02)\n",
      "Best mean reward updated 17.020\n",
      "431752:  233 games, mean reward 17.020, (epsilon 0.02)\n",
      "433743:  234 games, mean reward 17.060, (epsilon 0.02)\n",
      "Best mean reward updated 17.060\n",
      "435524:  235 games, mean reward 17.110, (epsilon 0.02)\n",
      "Best mean reward updated 17.110\n",
      "437266:  236 games, mean reward 17.140, (epsilon 0.02)\n",
      "Best mean reward updated 17.140\n",
      "439049:  237 games, mean reward 17.210, (epsilon 0.02)\n",
      "Best mean reward updated 17.210\n",
      "440958:  238 games, mean reward 17.260, (epsilon 0.02)\n",
      "Best mean reward updated 17.260\n",
      "442764:  239 games, mean reward 17.300, (epsilon 0.02)\n",
      "Best mean reward updated 17.300\n",
      "444950:  240 games, mean reward 17.250, (epsilon 0.02)\n",
      "446756:  241 games, mean reward 17.240, (epsilon 0.02)\n",
      "448476:  242 games, mean reward 17.300, (epsilon 0.02)\n",
      "450176:  243 games, mean reward 17.380, (epsilon 0.02)\n",
      "Best mean reward updated 17.380\n",
      "452313:  244 games, mean reward 17.360, (epsilon 0.02)\n",
      "454166:  245 games, mean reward 17.340, (epsilon 0.02)\n",
      "456009:  246 games, mean reward 17.320, (epsilon 0.02)\n",
      "457832:  247 games, mean reward 17.380, (epsilon 0.02)\n",
      "459898:  248 games, mean reward 17.380, (epsilon 0.02)\n",
      "461565:  249 games, mean reward 17.440, (epsilon 0.02)\n",
      "Best mean reward updated 17.440\n",
      "463449:  250 games, mean reward 17.460, (epsilon 0.02)\n",
      "Best mean reward updated 17.460\n",
      "465362:  251 games, mean reward 17.480, (epsilon 0.02)\n",
      "Best mean reward updated 17.480\n",
      "467243:  252 games, mean reward 17.580, (epsilon 0.02)\n",
      "Best mean reward updated 17.580\n",
      "468940:  253 games, mean reward 17.670, (epsilon 0.02)\n",
      "Best mean reward updated 17.670\n",
      "470692:  254 games, mean reward 17.690, (epsilon 0.02)\n",
      "Best mean reward updated 17.690\n",
      "472562:  255 games, mean reward 17.700, (epsilon 0.02)\n",
      "Best mean reward updated 17.700\n",
      "474229:  256 games, mean reward 17.750, (epsilon 0.02)\n",
      "Best mean reward updated 17.750\n",
      "476025:  257 games, mean reward 17.760, (epsilon 0.02)\n",
      "Best mean reward updated 17.760\n",
      "478145:  258 games, mean reward 17.680, (epsilon 0.02)\n",
      "480110:  259 games, mean reward 17.720, (epsilon 0.02)\n",
      "482384:  260 games, mean reward 17.680, (epsilon 0.02)\n",
      "484212:  261 games, mean reward 17.660, (epsilon 0.02)\n",
      "486230:  262 games, mean reward 17.640, (epsilon 0.02)\n",
      "488166:  263 games, mean reward 17.690, (epsilon 0.02)\n",
      "489927:  264 games, mean reward 17.700, (epsilon 0.02)\n",
      "491718:  265 games, mean reward 17.710, (epsilon 0.02)\n",
      "493436:  266 games, mean reward 17.700, (epsilon 0.02)\n",
      "495261:  267 games, mean reward 17.690, (epsilon 0.02)\n",
      "497116:  268 games, mean reward 17.700, (epsilon 0.02)\n",
      "498969:  269 games, mean reward 17.700, (epsilon 0.02)\n",
      "500784:  270 games, mean reward 17.690, (epsilon 0.02)\n",
      "502679:  271 games, mean reward 17.680, (epsilon 0.02)\n",
      "504347:  272 games, mean reward 17.700, (epsilon 0.02)\n",
      "506180:  273 games, mean reward 17.680, (epsilon 0.02)\n",
      "508178:  274 games, mean reward 17.710, (epsilon 0.02)\n",
      "510052:  275 games, mean reward 17.720, (epsilon 0.02)\n",
      "512291:  276 games, mean reward 17.750, (epsilon 0.02)\n",
      "514180:  277 games, mean reward 17.870, (epsilon 0.02)\n",
      "Best mean reward updated 17.870\n",
      "515880:  278 games, mean reward 17.920, (epsilon 0.02)\n",
      "Best mean reward updated 17.920\n",
      "517579:  279 games, mean reward 18.000, (epsilon 0.02)\n",
      "Best mean reward updated 18.000\n",
      "519350:  280 games, mean reward 17.980, (epsilon 0.02)\n",
      "521395:  281 games, mean reward 17.960, (epsilon 0.02)\n",
      "523176:  282 games, mean reward 17.940, (epsilon 0.02)\n",
      "525406:  283 games, mean reward 17.890, (epsilon 0.02)\n",
      "527545:  284 games, mean reward 17.920, (epsilon 0.02)\n",
      "529273:  285 games, mean reward 17.950, (epsilon 0.02)\n",
      "531002:  286 games, mean reward 17.960, (epsilon 0.02)\n",
      "532728:  287 games, mean reward 18.000, (epsilon 0.02)\n",
      "534659:  288 games, mean reward 17.990, (epsilon 0.02)\n",
      "536437:  289 games, mean reward 18.070, (epsilon 0.02)\n",
      "Best mean reward updated 18.070\n",
      "538373:  290 games, mean reward 18.050, (epsilon 0.02)\n",
      "540162:  291 games, mean reward 18.040, (epsilon 0.02)\n",
      "541860:  292 games, mean reward 18.050, (epsilon 0.02)\n",
      "543727:  293 games, mean reward 18.040, (epsilon 0.02)\n",
      "545395:  294 games, mean reward 18.040, (epsilon 0.02)\n",
      "547424:  295 games, mean reward 18.040, (epsilon 0.02)\n",
      "549337:  296 games, mean reward 18.080, (epsilon 0.02)\n",
      "Best mean reward updated 18.080\n",
      "551091:  297 games, mean reward 18.090, (epsilon 0.02)\n",
      "Best mean reward updated 18.090\n",
      "552853:  298 games, mean reward 18.090, (epsilon 0.02)\n",
      "554859:  299 games, mean reward 18.070, (epsilon 0.02)\n",
      "556554:  300 games, mean reward 18.070, (epsilon 0.02)\n",
      "558456:  301 games, mean reward 18.100, (epsilon 0.02)\n",
      "Best mean reward updated 18.100\n",
      "560274:  302 games, mean reward 18.120, (epsilon 0.02)\n",
      "Best mean reward updated 18.120\n",
      "562426:  303 games, mean reward 18.090, (epsilon 0.02)\n",
      "564590:  304 games, mean reward 18.070, (epsilon 0.02)\n",
      "566421:  305 games, mean reward 18.080, (epsilon 0.02)\n",
      "568457:  306 games, mean reward 18.110, (epsilon 0.02)\n",
      "570317:  307 games, mean reward 18.080, (epsilon 0.02)\n",
      "572363:  308 games, mean reward 18.040, (epsilon 0.02)\n",
      "574142:  309 games, mean reward 18.030, (epsilon 0.02)\n",
      "576234:  310 games, mean reward 17.990, (epsilon 0.02)\n",
      "578178:  311 games, mean reward 18.000, (epsilon 0.02)\n",
      "580082:  312 games, mean reward 18.010, (epsilon 0.02)\n",
      "582017:  313 games, mean reward 17.990, (epsilon 0.02)\n",
      "583709:  314 games, mean reward 18.040, (epsilon 0.02)\n",
      "585516:  315 games, mean reward 18.050, (epsilon 0.02)\n",
      "587178:  316 games, mean reward 18.040, (epsilon 0.02)\n",
      "588985:  317 games, mean reward 18.050, (epsilon 0.02)\n",
      "590941:  318 games, mean reward 18.080, (epsilon 0.02)\n",
      "592778:  319 games, mean reward 18.100, (epsilon 0.02)\n",
      "594777:  320 games, mean reward 18.120, (epsilon 0.02)\n",
      "596762:  321 games, mean reward 18.130, (epsilon 0.02)\n",
      "Best mean reward updated 18.130\n",
      "598429:  322 games, mean reward 18.160, (epsilon 0.02)\n",
      "Best mean reward updated 18.160\n",
      "600125:  323 games, mean reward 18.230, (epsilon 0.02)\n",
      "Best mean reward updated 18.230\n",
      "602192:  324 games, mean reward 18.230, (epsilon 0.02)\n",
      "604063:  325 games, mean reward 18.190, (epsilon 0.02)\n",
      "605829:  326 games, mean reward 18.220, (epsilon 0.02)\n",
      "607774:  327 games, mean reward 18.220, (epsilon 0.02)\n",
      "609598:  328 games, mean reward 18.230, (epsilon 0.02)\n",
      "611255:  329 games, mean reward 18.240, (epsilon 0.02)\n",
      "Best mean reward updated 18.240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613060:  330 games, mean reward 18.250, (epsilon 0.02)\n",
      "Best mean reward updated 18.250\n",
      "614908:  331 games, mean reward 18.250, (epsilon 0.02)\n",
      "616825:  332 games, mean reward 18.250, (epsilon 0.02)\n",
      "618775:  333 games, mean reward 18.250, (epsilon 0.02)\n",
      "620559:  334 games, mean reward 18.270, (epsilon 0.02)\n",
      "Best mean reward updated 18.270\n",
      "622475:  335 games, mean reward 18.260, (epsilon 0.02)\n",
      "624327:  336 games, mean reward 18.250, (epsilon 0.02)\n",
      "626270:  337 games, mean reward 18.240, (epsilon 0.02)\n",
      "628216:  338 games, mean reward 18.250, (epsilon 0.02)\n",
      "629945:  339 games, mean reward 18.260, (epsilon 0.02)\n",
      "631933:  340 games, mean reward 18.280, (epsilon 0.02)\n",
      "Best mean reward updated 18.280\n",
      "633828:  341 games, mean reward 18.290, (epsilon 0.02)\n",
      "Best mean reward updated 18.290\n",
      "635557:  342 games, mean reward 18.300, (epsilon 0.02)\n",
      "Best mean reward updated 18.300\n",
      "637262:  343 games, mean reward 18.290, (epsilon 0.02)\n",
      "639153:  344 games, mean reward 18.330, (epsilon 0.02)\n",
      "Best mean reward updated 18.330\n",
      "640902:  345 games, mean reward 18.360, (epsilon 0.02)\n",
      "Best mean reward updated 18.360\n",
      "643617:  346 games, mean reward 18.310, (epsilon 0.02)\n",
      "645365:  347 games, mean reward 18.330, (epsilon 0.02)\n",
      "647082:  348 games, mean reward 18.380, (epsilon 0.02)\n",
      "Best mean reward updated 18.380\n",
      "648955:  349 games, mean reward 18.370, (epsilon 0.02)\n",
      "650718:  350 games, mean reward 18.380, (epsilon 0.02)\n",
      "652500:  351 games, mean reward 18.400, (epsilon 0.02)\n",
      "Best mean reward updated 18.400\n",
      "654335:  352 games, mean reward 18.410, (epsilon 0.02)\n",
      "Best mean reward updated 18.410\n",
      "656127:  353 games, mean reward 18.380, (epsilon 0.02)\n",
      "657940:  354 games, mean reward 18.380, (epsilon 0.02)\n",
      "659699:  355 games, mean reward 18.380, (epsilon 0.02)\n",
      "661809:  356 games, mean reward 18.320, (epsilon 0.02)\n",
      "664079:  357 games, mean reward 18.260, (epsilon 0.02)\n",
      "666070:  358 games, mean reward 18.290, (epsilon 0.02)\n",
      "667799:  359 games, mean reward 18.310, (epsilon 0.02)\n",
      "669504:  360 games, mean reward 18.370, (epsilon 0.02)\n",
      "671235:  361 games, mean reward 18.380, (epsilon 0.02)\n",
      "673175:  362 games, mean reward 18.390, (epsilon 0.02)\n",
      "674873:  363 games, mean reward 18.410, (epsilon 0.02)\n",
      "676777:  364 games, mean reward 18.390, (epsilon 0.02)\n",
      "678570:  365 games, mean reward 18.410, (epsilon 0.02)\n",
      "680610:  366 games, mean reward 18.350, (epsilon 0.02)\n",
      "682932:  367 games, mean reward 18.280, (epsilon 0.02)\n",
      "684758:  368 games, mean reward 18.290, (epsilon 0.02)\n",
      "686580:  369 games, mean reward 18.320, (epsilon 0.02)\n",
      "688711:  370 games, mean reward 18.300, (epsilon 0.02)\n",
      "690442:  371 games, mean reward 18.320, (epsilon 0.02)\n",
      "692462:  372 games, mean reward 18.290, (epsilon 0.02)\n",
      "694717:  373 games, mean reward 18.240, (epsilon 0.02)\n",
      "696768:  374 games, mean reward 18.260, (epsilon 0.02)\n",
      "698927:  375 games, mean reward 18.250, (epsilon 0.02)\n",
      "700596:  376 games, mean reward 18.310, (epsilon 0.02)\n",
      "702305:  377 games, mean reward 18.330, (epsilon 0.02)\n",
      "704028:  378 games, mean reward 18.330, (epsilon 0.02)\n",
      "705751:  379 games, mean reward 18.320, (epsilon 0.02)\n",
      "707735:  380 games, mean reward 18.310, (epsilon 0.02)\n",
      "709618:  381 games, mean reward 18.350, (epsilon 0.02)\n",
      "711522:  382 games, mean reward 18.360, (epsilon 0.02)\n",
      "713177:  383 games, mean reward 18.420, (epsilon 0.02)\n",
      "Best mean reward updated 18.420\n",
      "714917:  384 games, mean reward 18.460, (epsilon 0.02)\n",
      "Best mean reward updated 18.460\n",
      "716622:  385 games, mean reward 18.440, (epsilon 0.02)\n",
      "718321:  386 games, mean reward 18.450, (epsilon 0.02)\n",
      "720329:  387 games, mean reward 18.420, (epsilon 0.02)\n",
      "722304:  388 games, mean reward 18.430, (epsilon 0.02)\n",
      "724002:  389 games, mean reward 18.450, (epsilon 0.02)\n",
      "725882:  390 games, mean reward 18.450, (epsilon 0.02)\n",
      "727640:  391 games, mean reward 18.470, (epsilon 0.02)\n",
      "Best mean reward updated 18.470\n",
      "729392:  392 games, mean reward 18.460, (epsilon 0.02)\n",
      "731274:  393 games, mean reward 18.460, (epsilon 0.02)\n",
      "732927:  394 games, mean reward 18.470, (epsilon 0.02)\n",
      "734639:  395 games, mean reward 18.500, (epsilon 0.02)\n",
      "Best mean reward updated 18.500\n",
      "736418:  396 games, mean reward 18.520, (epsilon 0.02)\n",
      "Best mean reward updated 18.520\n",
      "738242:  397 games, mean reward 18.510, (epsilon 0.02)\n",
      "740106:  398 games, mean reward 18.510, (epsilon 0.02)\n",
      "741894:  399 games, mean reward 18.550, (epsilon 0.02)\n",
      "Best mean reward updated 18.550\n",
      "743774:  400 games, mean reward 18.520, (epsilon 0.02)\n",
      "745604:  401 games, mean reward 18.530, (epsilon 0.02)\n",
      "747558:  402 games, mean reward 18.510, (epsilon 0.02)\n",
      "749343:  403 games, mean reward 18.530, (epsilon 0.02)\n",
      "751172:  404 games, mean reward 18.560, (epsilon 0.02)\n",
      "Best mean reward updated 18.560\n",
      "753386:  405 games, mean reward 18.510, (epsilon 0.02)\n",
      "755293:  406 games, mean reward 18.510, (epsilon 0.02)\n",
      "757158:  407 games, mean reward 18.530, (epsilon 0.02)\n",
      "758970:  408 games, mean reward 18.570, (epsilon 0.02)\n",
      "Best mean reward updated 18.570\n",
      "760886:  409 games, mean reward 18.570, (epsilon 0.02)\n",
      "762772:  410 games, mean reward 18.590, (epsilon 0.02)\n",
      "Best mean reward updated 18.590\n",
      "764591:  411 games, mean reward 18.600, (epsilon 0.02)\n",
      "Best mean reward updated 18.600\n",
      "766344:  412 games, mean reward 18.600, (epsilon 0.02)\n",
      "768014:  413 games, mean reward 18.620, (epsilon 0.02)\n",
      "Best mean reward updated 18.620\n",
      "769712:  414 games, mean reward 18.620, (epsilon 0.02)\n",
      "771523:  415 games, mean reward 18.610, (epsilon 0.02)\n",
      "773643:  416 games, mean reward 18.570, (epsilon 0.02)\n",
      "775359:  417 games, mean reward 18.570, (epsilon 0.02)\n",
      "777164:  418 games, mean reward 18.590, (epsilon 0.02)\n",
      "778899:  419 games, mean reward 18.600, (epsilon 0.02)\n",
      "780845:  420 games, mean reward 18.630, (epsilon 0.02)\n",
      "Best mean reward updated 18.630\n",
      "782638:  421 games, mean reward 18.630, (epsilon 0.02)\n",
      "784359:  422 games, mean reward 18.630, (epsilon 0.02)\n",
      "786170:  423 games, mean reward 18.620, (epsilon 0.02)\n",
      "788075:  424 games, mean reward 18.620, (epsilon 0.02)\n",
      "790110:  425 games, mean reward 18.630, (epsilon 0.02)\n",
      "791919:  426 games, mean reward 18.620, (epsilon 0.02)\n",
      "793825:  427 games, mean reward 18.600, (epsilon 0.02)\n",
      "795941:  428 games, mean reward 18.560, (epsilon 0.02)\n",
      "797772:  429 games, mean reward 18.550, (epsilon 0.02)\n",
      "799562:  430 games, mean reward 18.560, (epsilon 0.02)\n",
      "801414:  431 games, mean reward 18.540, (epsilon 0.02)\n",
      "803308:  432 games, mean reward 18.530, (epsilon 0.02)\n",
      "805086:  433 games, mean reward 18.540, (epsilon 0.02)\n",
      "807158:  434 games, mean reward 18.500, (epsilon 0.02)\n",
      "809196:  435 games, mean reward 18.500, (epsilon 0.02)\n",
      "811433:  436 games, mean reward 18.470, (epsilon 0.02)\n",
      "813181:  437 games, mean reward 18.480, (epsilon 0.02)\n",
      "814880:  438 games, mean reward 18.510, (epsilon 0.02)\n",
      "816645:  439 games, mean reward 18.500, (epsilon 0.02)\n",
      "818507:  440 games, mean reward 18.540, (epsilon 0.02)\n",
      "820260:  441 games, mean reward 18.550, (epsilon 0.02)\n",
      "822058:  442 games, mean reward 18.550, (epsilon 0.02)\n",
      "824291:  443 games, mean reward 18.510, (epsilon 0.02)\n",
      "826103:  444 games, mean reward 18.510, (epsilon 0.02)\n",
      "827853:  445 games, mean reward 18.510, (epsilon 0.02)\n",
      "830012:  446 games, mean reward 18.550, (epsilon 0.02)\n",
      "831707:  447 games, mean reward 18.560, (epsilon 0.02)\n",
      "833752:  448 games, mean reward 18.520, (epsilon 0.02)\n",
      "835566:  449 games, mean reward 18.520, (epsilon 0.02)\n",
      "837391:  450 games, mean reward 18.500, (epsilon 0.02)\n",
      "839087:  451 games, mean reward 18.510, (epsilon 0.02)\n",
      "840847:  452 games, mean reward 18.510, (epsilon 0.02)\n",
      "842775:  453 games, mean reward 18.530, (epsilon 0.02)\n",
      "844472:  454 games, mean reward 18.540, (epsilon 0.02)\n",
      "846253:  455 games, mean reward 18.540, (epsilon 0.02)\n",
      "848039:  456 games, mean reward 18.600, (epsilon 0.02)\n",
      "849801:  457 games, mean reward 18.660, (epsilon 0.02)\n",
      "Best mean reward updated 18.660\n",
      "851730:  458 games, mean reward 18.680, (epsilon 0.02)\n",
      "Best mean reward updated 18.680\n",
      "853588:  459 games, mean reward 18.670, (epsilon 0.02)\n",
      "855284:  460 games, mean reward 18.680, (epsilon 0.02)\n",
      "857096:  461 games, mean reward 18.670, (epsilon 0.02)\n",
      "858894:  462 games, mean reward 18.680, (epsilon 0.02)\n",
      "860592:  463 games, mean reward 18.680, (epsilon 0.02)\n",
      "862454:  464 games, mean reward 18.690, (epsilon 0.02)\n",
      "Best mean reward updated 18.690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864117:  465 games, mean reward 18.690, (epsilon 0.02)\n",
      "866088:  466 games, mean reward 18.730, (epsilon 0.02)\n",
      "Best mean reward updated 18.730\n",
      "868125:  467 games, mean reward 18.780, (epsilon 0.02)\n",
      "Best mean reward updated 18.780\n",
      "869822:  468 games, mean reward 18.800, (epsilon 0.02)\n",
      "Best mean reward updated 18.800\n",
      "871750:  469 games, mean reward 18.790, (epsilon 0.02)\n",
      "873441:  470 games, mean reward 18.840, (epsilon 0.02)\n",
      "Best mean reward updated 18.840\n",
      "875263:  471 games, mean reward 18.830, (epsilon 0.02)\n",
      "876929:  472 games, mean reward 18.860, (epsilon 0.02)\n",
      "Best mean reward updated 18.860\n",
      "878929:  473 games, mean reward 18.900, (epsilon 0.02)\n",
      "Best mean reward updated 18.900\n",
      "880626:  474 games, mean reward 18.910, (epsilon 0.02)\n",
      "Best mean reward updated 18.910\n",
      "882376:  475 games, mean reward 18.950, (epsilon 0.02)\n",
      "Best mean reward updated 18.950\n",
      "884436:  476 games, mean reward 18.920, (epsilon 0.02)\n",
      "886335:  477 games, mean reward 18.890, (epsilon 0.02)\n",
      "888083:  478 games, mean reward 18.890, (epsilon 0.02)\n",
      "889912:  479 games, mean reward 18.900, (epsilon 0.02)\n",
      "891612:  480 games, mean reward 18.920, (epsilon 0.02)\n",
      "893413:  481 games, mean reward 18.930, (epsilon 0.02)\n",
      "895169:  482 games, mean reward 18.940, (epsilon 0.02)\n",
      "897280:  483 games, mean reward 18.900, (epsilon 0.02)\n",
      "898977:  484 games, mean reward 18.910, (epsilon 0.02)\n",
      "900764:  485 games, mean reward 18.920, (epsilon 0.02)\n",
      "902530:  486 games, mean reward 18.910, (epsilon 0.02)\n",
      "904346:  487 games, mean reward 18.930, (epsilon 0.02)\n",
      "906121:  488 games, mean reward 18.940, (epsilon 0.02)\n",
      "907882:  489 games, mean reward 18.930, (epsilon 0.02)\n",
      "909693:  490 games, mean reward 18.940, (epsilon 0.02)\n",
      "911474:  491 games, mean reward 18.910, (epsilon 0.02)\n",
      "913467:  492 games, mean reward 18.870, (epsilon 0.02)\n",
      "915164:  493 games, mean reward 18.880, (epsilon 0.02)\n",
      "917069:  494 games, mean reward 18.840, (epsilon 0.02)\n",
      "918761:  495 games, mean reward 18.830, (epsilon 0.02)\n",
      "920423:  496 games, mean reward 18.830, (epsilon 0.02)\n",
      "922244:  497 games, mean reward 18.830, (epsilon 0.02)\n",
      "924002:  498 games, mean reward 18.840, (epsilon 0.02)\n",
      "926338:  499 games, mean reward 18.810, (epsilon 0.02)\n",
      "928672:  500 games, mean reward 18.770, (epsilon 0.02)\n",
      "930732:  501 games, mean reward 18.760, (epsilon 0.02)\n",
      "932519:  502 games, mean reward 18.770, (epsilon 0.02)\n",
      "934648:  503 games, mean reward 18.750, (epsilon 0.02)\n",
      "936847:  504 games, mean reward 18.740, (epsilon 0.02)\n",
      "938702:  505 games, mean reward 18.770, (epsilon 0.02)\n",
      "941053:  506 games, mean reward 18.700, (epsilon 0.02)\n",
      "942865:  507 games, mean reward 18.700, (epsilon 0.02)\n",
      "944563:  508 games, mean reward 18.710, (epsilon 0.02)\n",
      "946541:  509 games, mean reward 18.690, (epsilon 0.02)\n",
      "948195:  510 games, mean reward 18.720, (epsilon 0.02)\n",
      "950237:  511 games, mean reward 18.670, (epsilon 0.02)\n",
      "952172:  512 games, mean reward 18.610, (epsilon 0.02)\n",
      "953876:  513 games, mean reward 18.610, (epsilon 0.02)\n",
      "955541:  514 games, mean reward 18.600, (epsilon 0.02)\n",
      "957239:  515 games, mean reward 18.630, (epsilon 0.02)\n",
      "959016:  516 games, mean reward 18.670, (epsilon 0.02)\n",
      "960848:  517 games, mean reward 18.650, (epsilon 0.02)\n",
      "962655:  518 games, mean reward 18.660, (epsilon 0.02)\n",
      "964384:  519 games, mean reward 18.650, (epsilon 0.02)\n",
      "966113:  520 games, mean reward 18.670, (epsilon 0.02)\n",
      "967911:  521 games, mean reward 18.680, (epsilon 0.02)\n",
      "969861:  522 games, mean reward 18.680, (epsilon 0.02)\n",
      "971525:  523 games, mean reward 18.680, (epsilon 0.02)\n",
      "973219:  524 games, mean reward 18.720, (epsilon 0.02)\n",
      "974937:  525 games, mean reward 18.730, (epsilon 0.02)\n",
      "976631:  526 games, mean reward 18.750, (epsilon 0.02)\n",
      "978380:  527 games, mean reward 18.780, (epsilon 0.02)\n",
      "980328:  528 games, mean reward 18.820, (epsilon 0.02)\n",
      "982593:  529 games, mean reward 18.760, (epsilon 0.02)\n",
      "984392:  530 games, mean reward 18.740, (epsilon 0.02)\n",
      "986432:  531 games, mean reward 18.730, (epsilon 0.02)\n",
      "988185:  532 games, mean reward 18.750, (epsilon 0.02)\n",
      "989995:  533 games, mean reward 18.760, (epsilon 0.02)\n",
      "991833:  534 games, mean reward 18.800, (epsilon 0.02)\n",
      "993577:  535 games, mean reward 18.810, (epsilon 0.02)\n",
      "995243:  536 games, mean reward 18.860, (epsilon 0.02)\n",
      "996963:  537 games, mean reward 18.860, (epsilon 0.02)\n",
      "998715:  538 games, mean reward 18.850, (epsilon 0.02)\n",
      "1000606:  539 games, mean reward 18.850, (epsilon 0.02)\n",
      "1002400:  540 games, mean reward 18.860, (epsilon 0.02)\n",
      "1004295:  541 games, mean reward 18.840, (epsilon 0.02)\n",
      "1006057:  542 games, mean reward 18.840, (epsilon 0.02)\n",
      "1007866:  543 games, mean reward 18.870, (epsilon 0.02)\n",
      "1009679:  544 games, mean reward 18.880, (epsilon 0.02)\n",
      "1011579:  545 games, mean reward 18.850, (epsilon 0.02)\n",
      "1013452:  546 games, mean reward 18.870, (epsilon 0.02)\n",
      "1015499:  547 games, mean reward 18.830, (epsilon 0.02)\n",
      "1017294:  548 games, mean reward 18.870, (epsilon 0.02)\n",
      "1019240:  549 games, mean reward 18.850, (epsilon 0.02)\n",
      "1020989:  550 games, mean reward 18.870, (epsilon 0.02)\n",
      "1022895:  551 games, mean reward 18.820, (epsilon 0.02)\n",
      "1024758:  552 games, mean reward 18.810, (epsilon 0.02)\n",
      "1026753:  553 games, mean reward 18.780, (epsilon 0.02)\n",
      "1028543:  554 games, mean reward 18.760, (epsilon 0.02)\n",
      "1030208:  555 games, mean reward 18.770, (epsilon 0.02)\n",
      "1031936:  556 games, mean reward 18.760, (epsilon 0.02)\n",
      "1033665:  557 games, mean reward 18.750, (epsilon 0.02)\n",
      "1035360:  558 games, mean reward 18.780, (epsilon 0.02)\n",
      "1037111:  559 games, mean reward 18.780, (epsilon 0.02)\n",
      "1038917:  560 games, mean reward 18.760, (epsilon 0.02)\n",
      "1040885:  561 games, mean reward 18.740, (epsilon 0.02)\n",
      "1042615:  562 games, mean reward 18.740, (epsilon 0.02)\n",
      "1044457:  563 games, mean reward 18.710, (epsilon 0.02)\n",
      "1046336:  564 games, mean reward 18.700, (epsilon 0.02)\n",
      "1048389:  565 games, mean reward 18.660, (epsilon 0.02)\n",
      "1050484:  566 games, mean reward 18.660, (epsilon 0.02)\n",
      "1052150:  567 games, mean reward 18.700, (epsilon 0.02)\n",
      "1054301:  568 games, mean reward 18.660, (epsilon 0.02)\n",
      "1055967:  569 games, mean reward 18.670, (epsilon 0.02)\n",
      "1057631:  570 games, mean reward 18.660, (epsilon 0.02)\n",
      "1059476:  571 games, mean reward 18.670, (epsilon 0.02)\n",
      "1061592:  572 games, mean reward 18.640, (epsilon 0.02)\n",
      "1063525:  573 games, mean reward 18.650, (epsilon 0.02)\n",
      "1065319:  574 games, mean reward 18.630, (epsilon 0.02)\n",
      "1067124:  575 games, mean reward 18.610, (epsilon 0.02)\n",
      "1068975:  576 games, mean reward 18.630, (epsilon 0.02)\n",
      "1070794:  577 games, mean reward 18.650, (epsilon 0.02)\n",
      "1072460:  578 games, mean reward 18.650, (epsilon 0.02)\n",
      "1074426:  579 games, mean reward 18.620, (epsilon 0.02)\n",
      "1076291:  580 games, mean reward 18.600, (epsilon 0.02)\n",
      "1078367:  581 games, mean reward 18.570, (epsilon 0.02)\n",
      "1080218:  582 games, mean reward 18.570, (epsilon 0.02)\n",
      "1082530:  583 games, mean reward 18.570, (epsilon 0.02)\n",
      "1084533:  584 games, mean reward 18.550, (epsilon 0.02)\n",
      "1086583:  585 games, mean reward 18.540, (epsilon 0.02)\n",
      "1088470:  586 games, mean reward 18.540, (epsilon 0.02)\n",
      "1090391:  587 games, mean reward 18.530, (epsilon 0.02)\n",
      "1092225:  588 games, mean reward 18.530, (epsilon 0.02)\n",
      "1094200:  589 games, mean reward 18.520, (epsilon 0.02)\n",
      "1095925:  590 games, mean reward 18.530, (epsilon 0.02)\n",
      "1097674:  591 games, mean reward 18.550, (epsilon 0.02)\n",
      "1099787:  592 games, mean reward 18.540, (epsilon 0.02)\n",
      "1101581:  593 games, mean reward 18.540, (epsilon 0.02)\n",
      "1103443:  594 games, mean reward 18.560, (epsilon 0.02)\n",
      "1105235:  595 games, mean reward 18.560, (epsilon 0.02)\n",
      "1107005:  596 games, mean reward 18.540, (epsilon 0.02)\n",
      "1108917:  597 games, mean reward 18.550, (epsilon 0.02)\n",
      "1110777:  598 games, mean reward 18.530, (epsilon 0.02)\n",
      "1112442:  599 games, mean reward 18.560, (epsilon 0.02)\n",
      "1114218:  600 games, mean reward 18.620, (epsilon 0.02)\n",
      "1116080:  601 games, mean reward 18.620, (epsilon 0.02)\n",
      "1117746:  602 games, mean reward 18.640, (epsilon 0.02)\n",
      "1119504:  603 games, mean reward 18.670, (epsilon 0.02)\n",
      "1121170:  604 games, mean reward 18.690, (epsilon 0.02)\n",
      "1122836:  605 games, mean reward 18.720, (epsilon 0.02)\n",
      "1124544:  606 games, mean reward 18.820, (epsilon 0.02)\n",
      "1126237:  607 games, mean reward 18.840, (epsilon 0.02)\n",
      "1127906:  608 games, mean reward 18.830, (epsilon 0.02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1129619:  609 games, mean reward 18.880, (epsilon 0.02)\n",
      "1131495:  610 games, mean reward 18.860, (epsilon 0.02)\n",
      "1133401:  611 games, mean reward 18.910, (epsilon 0.02)\n",
      "1135097:  612 games, mean reward 18.980, (epsilon 0.02)\n",
      "Best mean reward updated 18.980\n",
      "1137080:  613 games, mean reward 18.970, (epsilon 0.02)\n",
      "1138962:  614 games, mean reward 18.960, (epsilon 0.02)\n",
      "1141043:  615 games, mean reward 18.940, (epsilon 0.02)\n",
      "1142694:  616 games, mean reward 18.950, (epsilon 0.02)\n",
      "1144387:  617 games, mean reward 18.980, (epsilon 0.02)\n",
      "1146335:  618 games, mean reward 18.970, (epsilon 0.02)\n",
      "1148222:  619 games, mean reward 18.960, (epsilon 0.02)\n",
      "1150450:  620 games, mean reward 18.950, (epsilon 0.02)\n",
      "1152514:  621 games, mean reward 18.930, (epsilon 0.02)\n",
      "1154250:  622 games, mean reward 18.930, (epsilon 0.02)\n",
      "1156113:  623 games, mean reward 18.900, (epsilon 0.02)\n",
      "1157884:  624 games, mean reward 18.870, (epsilon 0.02)\n",
      "1159777:  625 games, mean reward 18.860, (epsilon 0.02)\n",
      "1161658:  626 games, mean reward 18.840, (epsilon 0.02)\n",
      "1163593:  627 games, mean reward 18.820, (epsilon 0.02)\n",
      "1165656:  628 games, mean reward 18.790, (epsilon 0.02)\n",
      "1167434:  629 games, mean reward 18.840, (epsilon 0.02)\n",
      "1169291:  630 games, mean reward 18.860, (epsilon 0.02)\n",
      "1171019:  631 games, mean reward 18.890, (epsilon 0.02)\n",
      "1172826:  632 games, mean reward 18.890, (epsilon 0.02)\n",
      "1174616:  633 games, mean reward 18.880, (epsilon 0.02)\n",
      "1176749:  634 games, mean reward 18.850, (epsilon 0.02)\n",
      "1178681:  635 games, mean reward 18.850, (epsilon 0.02)\n",
      "1180448:  636 games, mean reward 18.840, (epsilon 0.02)\n",
      "1182245:  637 games, mean reward 18.830, (epsilon 0.02)\n",
      "1183898:  638 games, mean reward 18.840, (epsilon 0.02)\n",
      "1185791:  639 games, mean reward 18.830, (epsilon 0.02)\n",
      "1188010:  640 games, mean reward 18.800, (epsilon 0.02)\n",
      "1190136:  641 games, mean reward 18.780, (epsilon 0.02)\n",
      "1192023:  642 games, mean reward 18.780, (epsilon 0.02)\n",
      "1193678:  643 games, mean reward 18.800, (epsilon 0.02)\n",
      "1195352:  644 games, mean reward 18.810, (epsilon 0.02)\n",
      "1197257:  645 games, mean reward 18.840, (epsilon 0.02)\n",
      "1199097:  646 games, mean reward 18.840, (epsilon 0.02)\n",
      "1200979:  647 games, mean reward 18.860, (epsilon 0.02)\n",
      "1202633:  648 games, mean reward 18.870, (epsilon 0.02)\n",
      "1204381:  649 games, mean reward 18.900, (epsilon 0.02)\n",
      "1206034:  650 games, mean reward 18.910, (epsilon 0.02)\n",
      "1207870:  651 games, mean reward 18.950, (epsilon 0.02)\n",
      "1209980:  652 games, mean reward 18.940, (epsilon 0.02)\n",
      "1212043:  653 games, mean reward 18.960, (epsilon 0.02)\n",
      "1213699:  654 games, mean reward 18.980, (epsilon 0.02)\n",
      "1215867:  655 games, mean reward 18.950, (epsilon 0.02)\n",
      "1217522:  656 games, mean reward 18.970, (epsilon 0.02)\n",
      "1219359:  657 games, mean reward 18.970, (epsilon 0.02)\n",
      "1221184:  658 games, mean reward 18.960, (epsilon 0.02)\n",
      "1222840:  659 games, mean reward 18.980, (epsilon 0.02)\n",
      "1224494:  660 games, mean reward 19.000, (epsilon 0.02)\n",
      "Best mean reward updated 19.000\n",
      "1226284:  661 games, mean reward 19.020, (epsilon 0.02)\n",
      "Best mean reward updated 19.020\n",
      "Solved in 1226284 frames!\n"
     ]
    }
   ],
   "source": [
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    " \n",
    "buffer = ExperienceReplay(replay_size)\n",
    "agent = Agent(env, buffer)\n",
    "\n",
    "epsilon = eps_start\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "total_rewards = []\n",
    "frame_idx = 0  \n",
    "\n",
    "best_mean_reward = None\n",
    "\n",
    "while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(epsilon*eps_decay, eps_min)\n",
    "\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "\n",
    "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
    "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "            \n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "                best_mean_reward = mean_reward\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
    "\n",
    "            if mean_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < replay_start_size:\n",
    "            continue\n",
    "\n",
    "        batch = buffer.sample(batch_size)\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "        states_v = torch.tensor(states).to(device)\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        actions_v = torch.tensor(actions).to(device)\n",
    "        rewards_v = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        next_state_values = target_net(next_states_v).max(1)[0]\n",
    "\n",
    "        next_state_values[done_mask] = 0.0\n",
    "\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
    "\n",
    "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if frame_idx % sync_target_frames == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "       \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZPkszw66cmO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>Training ends at  2021-04-21 01:05:43.929660\n"
     ]
    }
   ],
   "source": [
    "print(\">>>Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNH2N64k3QRz"
   },
   "source": [
    "Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKbcwfK321Hl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8076a9f2beccdd5b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8076a9f2beccdd5b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard  --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6p0jvxoC3m5W"
   },
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLEfbkKl6AZV"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import collections\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4m0Vm4Yp91ZI"
   },
   "source": [
    "Tunning the image rendering in colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kgpHXywd5SyZ"
   },
   "outputs": [],
   "source": [
    "# Taken from \n",
    "# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
    "\n",
    "# !apt-get install -y xvfb x11-utils\n",
    "\n",
    "# !pip install pyvirtualdisplay==0.2.* \\\n",
    "#             PyOpenGL==3.1.* \\\n",
    "#             PyOpenGL-accelerate==3.1.*\n",
    "\n",
    "# !pip install gym[box2d]==0.17.*\n",
    "\n",
    "import pyvirtualdisplay\n",
    "\n",
    "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "_ = _display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvN4S8R53mJI"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bc4a81046c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEFAULT_ENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecord_folder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_env' is not defined"
     ]
    }
   ],
   "source": [
    "# Taken (partially) from \n",
    "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
    "\n",
    "\n",
    "model='PongNoFrameskip-v4-best.dat'\n",
    "record_folder=\"test\"  \n",
    "visualize=True\n",
    "\n",
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "if record_folder:\n",
    "        env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
    "net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "        start_ts = time.time()\n",
    "        if visualize:\n",
    "            env.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        if visualize:\n",
    "            delta = 1/FPS - (time.time() - start_ts)\n",
    "            if delta > 0:\n",
    "                time.sleep(delta)\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "\n",
    "if record_folder:\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DRL_15_16_17_DQN_Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
